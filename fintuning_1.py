import os
import ssl
import requests
import torch
import librosa
import pandas as pd
from dataclasses import dataclass
from typing import Any, Dict, List, Union
from datasets import Dataset
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    GenerationConfig,
)
from urllib3.exceptions import InsecureRequestWarning

# 1. SSL ë° ë³´ì•ˆ ì„¤ì • (ë³´ì•ˆë§ í™˜ê²½ ëŒ€ì‘)
os.environ["CURL_CA_BUNDLE"] = ""
os.environ["PYTHONHTTPSVERIFY"] = "0"
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

# 2. ê²½ë¡œ ì„¤ì •
BASE_DIR = "./train_data"
MODEL_PATH = "./whisper-tiny-local"
OUTPUT_DIR = "./whisper-tiny-finetuned"


# 3. Whisper ì „ìš© ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # ì…ë ¥ ì˜¤ë””ì˜¤ íŠ¹ì§•(input_features) íŒ¨ë”©
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )

        # ë¼ë²¨(labels) íŒ¨ë”©
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # ì†ì‹¤ ê³„ì‚° ì‹œ íŒ¨ë”© í† í° ë¬´ì‹œ (-100)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        batch["labels"] = labels
        return batch


# 4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì „ì²˜ë¦¬ í•¨ìˆ˜ (Batched)
def prepare_dataset_batched(batch, processor):
    # ì˜¤ë””ì˜¤ ê²½ë¡œ ìƒì„± ë° ë¡œë“œ
    audio_paths = [os.path.join(BASE_DIR, "audio", f) for f in batch["file_name"]]
    # librosaë¡œ 16kHz ë¡œë“œ (ì§ì ‘ ë””ì½”ë”©í•˜ì—¬ torchcodec ì—ëŸ¬ ë°©ì§€)
    speech_list = [librosa.load(p, sr=16000)[0] for p in audio_paths]

    # íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
    inputs = processor.feature_extractor(speech_list, sampling_rate=16000)
    batch["input_features"] = inputs.input_features

    # í…ìŠ¤íŠ¸ í† í°í™” (metadata.csvì˜ 'text' ì»¬ëŸ¼ ì‚¬ìš©)
    batch["labels"] = processor.tokenizer(batch["text"]).input_ids
    return batch


def main():
    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“¦ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    if not os.path.exists(os.path.join(BASE_DIR, "metadata.csv")):
        print("âŒ metadata.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.read_csv(os.path.join(BASE_DIR, "metadata.csv"))  #
    dataset = Dataset.from_pandas(df)

    # 6. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
    print("ğŸ¤– ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    processor = WhisperProcessor.from_pretrained(MODEL_PATH, local_files_only=True)
    model = WhisperForConditionalGeneration.from_pretrained(
        MODEL_PATH, local_files_only=True
    )

    # 7. ì „ì²˜ë¦¬ ì ìš© (RAM ì ˆì•½ ëª¨ë“œ)
    print("ğŸ§¹ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (Batched)...")
    dataset = dataset.map(
        prepare_dataset_batched,
        batched=True,
        batch_size=16,  # ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¼ ì¡°ì ˆ
        fn_kwargs={"processor": processor},
        remove_columns=dataset.column_names,
        num_proc=1,
        keep_in_memory=False,  # ë””ìŠ¤í¬ ìºì‹œ í™œìš©
    )

    # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (9:1)
    dataset = dataset.train_test_split(test_size=0.1)

    # 8. ì½œë ˆì´í„° ë° í•™ìŠµ ì„¤ì •
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=4,  # VRAM ë¶€ì¡± ì‹œ 2ë¡œ ë‚®ì¶¤
        gradient_accumulation_steps=4,
        learning_rate=1e-5,
        warmup_steps=1,
        max_steps=10,  # ì „ì²´ í•™ìŠµ ë£¨í”„ íšŸìˆ˜
        gradient_checkpointing=True,
        fp16=True,  # GPU ì‚¬ìš© ì‹œ True
        eval_strategy="steps",
        predict_with_generate=True,
        generation_max_length=225,
        save_steps=200,
        eval_steps=200,
        logging_steps=1,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
    )

    # 9. íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰
    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        data_collator=data_collator,
        # tokenizer=processor.feature_extractor,
    )

    print("ğŸš€ í•™ìŠµ ì‹œì‘...")
    trainer.train()

    # 3. ğŸ”¥ í•µì‹¬: ëª¨ë“  ì„¤ì • íŒŒì¼ì„ ì™„ë²½í•˜ê²Œ ì €ì¥
    print("ğŸ’¾ ëª¨ë¸ ë° ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘...")
    trainer.save_model(OUTPUT_DIR)
    processor.save_pretrained(OUTPUT_DIR)

    # ìµœì‹  ê·œê²©ì˜ Generation Configë¥¼ ìƒì„±í•´ì„œ í•¨ê»˜ ì €ì¥
    gen_config = GenerationConfig.from_model_config(model.config)
    gen_config.update(language="korean", task="transcribe")
    gen_config.save_pretrained(OUTPUT_DIR)

    print(f"âœ… ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
