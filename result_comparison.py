import torch
import librosa
import os
from transformers import WhisperForConditionalGeneration, WhisperProcessor, GenerationConfig

# 1. ê²½ë¡œ ì„¤ì •
original_model_path = "./whisper-tiny-local"
finetuned_model_path = "./whisper-tiny-finetuned"
audio_files = ["test_1.m4a", "test_2.m4a", "test_3.m4a"]

device = "mps" if torch.backends.mps.is_available() else "cpu"

def get_transcription(model_path, files):
    print(f"\n[{model_path}] ëª¨ë¸ ë¡œë”© ë° ì„¤ì •...")
    
    model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)
    processor = WhisperProcessor.from_pretrained(model_path)
    
    # â­ï¸ í•´ê²°ì±…: ëª¨ë“  ê¸°ì¡´ ì„¤ì •ì„ ë¬´ì‹œí•˜ê³  í•œêµ­ì–´ ì „ìš© ì„¤ì •ì„ ìƒˆë¡œ ìƒì„±
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    
    results = {}
    for audio_path in files:
        if not os.path.exists(audio_path):
            continue
            
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            audio_array, _ = librosa.load(audio_path, sr=16000)
            input_features = processor(audio_array, sampling_rate=16000, return_tensors="pt").input_features.to(device)
            
            # â­ï¸ ë¹” ì„œì¹˜(beam search) ì ìš©ìœ¼ë¡œ ì¸ì‹ë¥  ìƒí–¥ ë° í•œêµ­ì–´ ê³ ì •
            predicted_ids = model.generate(
                input_features,
                forced_decoder_ids=forced_decoder_ids,
                num_beams=5,            # 5ê°œì˜ í›„ë³´êµ° ì¤‘ ìµœì ì„ ì„ íƒ (ì •í™•ë„ í–¥ìƒ)
                no_repeat_ngram_size=2, # ë°˜ë³µë˜ëŠ” í—›ì†Œë¦¬ ë°©ì§€
                max_length=225
            )
            
            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            results[audio_path] = transcription
            
        except Exception as e:
            results[audio_path] = f"ì—ëŸ¬: {e}"
            
    del model, processor
    if device == "mps": torch.mps.empty_cache()
    return results

# 2. ì‹¤í–‰
print("--- [ê²€ì¦] íŒŒì¸íŠœë‹ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤ ---")
original_results = get_transcription(original_model_path, audio_files)
finetuned_results = get_transcription(finetuned_model_path, audio_files)

# 3. ê²°ê³¼ ì¶œë ¥
print(f"\n{'='*35} ìµœì¢… ë¹„êµ ê²°ê³¼ {'='*35}")
for audio in audio_files:
    print(f"\n[íŒŒì¼ëª…: {audio}]")
    # ì›ë³¸ì´ ì˜ì–´ í™˜ê°ì„ ë±‰ì„ ë•Œ íŒŒì¸íŠœë‹ì´ í•œêµ­ì–´ë¥¼ ì¡ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!
    print(f"  ğŸ‡°ğŸ‡· ì›ë³¸ (Original) : {original_results.get(audio)}")
    print(f"  ğŸ”¥ íŒŒì¸íŠœë‹ (FT)    : {finetuned_results.get(audio)}")
    print("-" * 75)